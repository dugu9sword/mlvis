{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference with Gumbel Softmax\n",
    "\n",
    "*\\[Reference\\] Learning to Explain: An Information-Theoretic Perspective on Model Interpretation, https://github.com/Jianbo-Lab/L2X*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标：$max_\\mathcal{E} I(X_S, Y)$, s.t. $S \\in \\mathcal{E}(X) $\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\mathbb{E} [I(X_S, Y)] \\\\\n",
    "= & \\mathbb{E} [ \\frac{P(X_S, Y)}{P(X_S) \\cdot P(Y)} ] \\\\\n",
    "= & \\mathbb{E} [ P(Y|X_S) ] + const \\\\ \n",
    "= & \\mathbb{E}_{X} \\mathbb{E}_{X_S|X} \\mathbb{E}_{Y|X_S} [ P(Y|X_S) ] + const\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\mathbb{E}_{X}$：从真实数据分布中采样\n",
    "\n",
    "- $\\mathbb{E}_{X_S|X}$：$\\mathcal{E}$ 可以看作一个 proposal，对于 $X$ 返回一个离散的子集合 $X_S$，也就是说 $\\mathcal{E}(X_S|X)$ 是一个 dirac 分布。$KL(\\mathcal{E}(X_S|X)|P(X_S))$ 在 $P(X_S)$ 具有 uniform 先验的情况下，是一个常数。因此可以直接通过 Monte Carlo 采样来优化 ELBO。\n",
    "    - 由于 $\\mathcal{E}$ 不可导，因此可以使用 gumbel softmax 进行近似。\n",
    "\n",
    "- $\\mathbb{E}_{Y|X_S} [ P(Y|X_S) ] \\ge \\mathbb{E}_{Y|X_S} [ Q(Y|X_S) ]$：使用 $Q$ 进行 Variational Inference，获取并优化 ELBO。（存疑？）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu(sth):\n",
    "    if torch.cuda.is_available():\n",
    "        if isinstance(sth, tuple) or isinstance(sth, list):\n",
    "            ret = [gpu(ele) for ele in sth]\n",
    "        else:\n",
    "            ret = sth.cuda()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(IterableDataset):\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            x = torch.randint(1, 10, (9, ))\n",
    "            y = x[0]\n",
    "            x[:5] = y\n",
    "            x = x[torch.randperm(9)]\n",
    "            yield x, y\n",
    "\n",
    "data = DataLoader(CustomDataset(), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feat(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(10, 32, padding_idx=0)\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=32, nhead=4), \n",
    "        num_layers=1)\n",
    "\n",
    "    def forward(self, idxs):\n",
    "        h = self.embed(idxs)\n",
    "        h = self.transformer(h)\n",
    "        return h\n",
    "\n",
    "# Q(Y|X_S)\n",
    "class Q(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clf = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        return self.clf(hidden)\n",
    "\n",
    "# X_S ~ E(X)\n",
    "class E(torch.nn.Module):\n",
    "    def __init__(self, tau, k):\n",
    "        super().__init__()\n",
    "        # B x T x 25\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 1),\n",
    "            torch.nn.Flatten(-2, -1)  # remove last dim\n",
    "        )\n",
    "        self.tau = tau\n",
    "        self.k = k\n",
    "    \n",
    "    def forward(self, hidden, hard=False):\n",
    "        logits = self.mlp(hidden)\n",
    "        logits = F.log_softmax(logits)\n",
    "        if not hard:\n",
    "            # probs = F.softmax(logits, dim=1)\n",
    "            # ind = torch.zeros_like(probs).scatter(1, probs.topk(self.k)[1], 1.)\n",
    "            # ret = probs + (ind - probs).detach()\n",
    "\n",
    "            logits_k = logits.unsqueeze(1).expand(logits.shape[0], self.k, logits.shape[1])\n",
    "            probs = F.gumbel_softmax(logits_k, tau=self.tau)\n",
    "            ret = probs.max(1)[0]\n",
    "        else:\n",
    "            ret = torch.zeros_like(logits)\n",
    "            ret.scatter_(1, logits.topk(self.k).indices, 1)\n",
    "        return ret\n",
    "\n",
    "# hidden: B x T x H\n",
    "# probs : B x T\n",
    "def pool(hidden, probs=None, k=None):\n",
    "    if probs is None:\n",
    "        return hidden.mean(1)\n",
    "    else:\n",
    "        hidden = probs.unsqueeze(-1) * hidden\n",
    "        return hidden.sum(1) / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3.], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = gpu(next(iter(data)))\n",
    "e(feat(x)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1 2.3150339126586914\n",
      "100 0.1 1.1462016105651855\n",
      "200 0.1 0.8118592500686646\n",
      "300 0.1 0.7601400017738342\n",
      "400 0.1 0.55384761095047\n",
      "500 0.1 0.7607303261756897\n",
      "600 0.1 0.651654839515686\n",
      "700 0.1 0.7534162402153015\n",
      "800 0.1 0.573569655418396\n",
      "900 0.1 0.6712555885314941\n",
      "1000 0.1 0.6235792636871338\n"
     ]
    }
   ],
   "source": [
    "feat = gpu(Feat())\n",
    "q = gpu(Q())\n",
    "e = gpu(E(0.1, 3))\n",
    "opt = torch.optim.AdamW(\n",
    "    itertools.chain(feat.parameters(), q.parameters(), e.parameters()), \n",
    "    5e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "feat.train()\n",
    "q.train()\n",
    "e.train()\n",
    "\n",
    "for bid, batch in enumerate(data):\n",
    "    # e.tau = max(0.1, (1000 - bid) / 1000 * 0.3)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    x, y = gpu(batch)\n",
    "\n",
    "    h = feat(x)\n",
    "\n",
    "    # loss1 = F.cross_entropy(pool(h), y)\n",
    "\n",
    "    probs = e(h, hard=False)\n",
    "    logits = q(pool(h, probs, probs.sum(1, keepdims=True)))\n",
    "    loss2 = F.cross_entropy(logits, y)\n",
    "\n",
    "    # loss = 0.5 * loss1 + loss2\n",
    "    loss = loss2\n",
    "\n",
    "    if bid % 100 == 0:\n",
    "        print(bid, e.tau, loss.item())\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if bid == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8, 3, 1, 1, 9, 1, 1, 1, 1],\n",
      "        [4, 7, 7, 6, 6, 6, 6, 9, 6],\n",
      "        [3, 7, 8, 7, 9, 9, 7, 7, 7],\n",
      "        [6, 8, 8, 8, 1, 7, 2, 8, 8],\n",
      "        [2, 5, 9, 9, 9, 9, 9, 2, 9]], device='cuda:0')\n",
      "\n",
      "mean >>>\n",
      "pred: tensor([1, 6, 7, 8, 9], device='cuda:0') tensor([1, 6, 7, 8, 9], device='cuda:0')\n",
      "\n",
      "explained(soft) >>>\n",
      "tensor([2.9987, 2.0110, 2.9154, 2.0000, 2.9857], device='cuda:0')\n",
      "pred: tensor([1, 6, 7, 8, 9], device='cuda:0') tensor([1, 6, 7, 8, 9], device='cuda:0')\n",
      "\n",
      "explained(hard) >>>\n",
      "pred: tensor([3, 6, 7, 6, 2], device='cuda:0') tensor([1, 6, 7, 8, 9], device='cuda:0')\n",
      "\n",
      "explained\n",
      "tensor([[0, 3, 0, 0, 9, 1, 0, 0, 0],\n",
      "        [0, 7, 0, 0, 6, 0, 6, 0, 0],\n",
      "        [3, 7, 0, 0, 0, 0, 7, 0, 0],\n",
      "        [6, 0, 0, 0, 0, 7, 2, 0, 0],\n",
      "        [2, 0, 0, 0, 9, 0, 0, 2, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "feat.eval()\n",
    "q.eval()\n",
    "e.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, y = gpu(next(iter(data)))\n",
    "    x = x[:5]\n",
    "    y = y[:5]\n",
    "\n",
    "    h = feat(x)\n",
    "    print(x)\n",
    "    # print(hard_idx)\n",
    "\n",
    "    # mean\n",
    "    print(\"\\nmean >>>\")\n",
    "    logits = q(pool(h))\n",
    "    print(\"pred:\", logits.argmax(1), y)\n",
    "\n",
    "    # estimated\n",
    "    print(\"\\nexplained(soft) >>>\")\n",
    "    probs = e(h, hard=False)\n",
    "    print(probs.sum(1))\n",
    "    logits = q(pool(h, probs, e.k))\n",
    "    print(\"pred:\", logits.argmax(1), y)\n",
    "\n",
    "    # estimated\n",
    "    print(\"\\nexplained(hard) >>>\")\n",
    "    probs = e(h, hard=True)\n",
    "    logits = q(pool(h, probs, e.k))\n",
    "    print(\"pred:\", logits.argmax(1), y)\n",
    "\n",
    "    # explain\n",
    "    print(\"\\nexplained\")\n",
    "    hard_idx = e(h, hard=True)\n",
    "    explained = x.masked_fill(~(hard_idx.bool()), 0)\n",
    "    print(explained)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
