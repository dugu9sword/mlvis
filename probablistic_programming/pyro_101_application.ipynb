{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import AbstractContextManager\n",
    "import rich\n",
    "import torch\n",
    "import numpy as np\n",
    "import pyro\n",
    "from pyro import distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer import Trace_ELBO, TraceEnum_ELBO, SVI\n",
    "from pyro.infer import config_enumerate, infer_discrete\n",
    "from pyro.distributions import constraints\n",
    "from pyro.ops.indexing import Vindex\n",
    "from pyro.infer.autoguide import AutoNormal, AutoDelta, AutoGuide\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class catch(AbstractContextManager):\n",
    "    def __init__(self, info = None):\n",
    "        self.info = info\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.info is not None:\n",
    "            print(f\"=== {self.info} ===\")\n",
    "\n",
    "    def __exit__(self, exctype, excinst, exctb):\n",
    "        if exctype is not None:\n",
    "            print(f\"Error: {exctype}\")\n",
    "        return True\n",
    "\n",
    "\n",
    "def render_model_plus(model, model_args=None):\n",
    "    display.display(\n",
    "        pyro.render_model(model, model_args, render_params=True, render_distributions=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Gaussian Distribution\n",
    "\n",
    "In this section, we use `pyro` to fit a Gaussian Distribution. This is a very toy example to get understanding of some primitives: `sample`, `param`, `model` and `guide`.\n",
    "\n",
    "### Difference between `sample` and `param`\n",
    "\n",
    "- connection with `model` and `guide`:\n",
    "  - `sample` (with the same name) must appear both in `model` and `guide`\n",
    "  - `param` can be `model`-only or `guide`-only\n",
    "- trainbale? if the arg of `sample` is:\n",
    "  - constant distribution: never change during training\n",
    "  - `param`eterized distribution: changed during training\n",
    "\n",
    "### What is trained\n",
    "\n",
    "See the case *model_3*, the `model` views `loc` and `scale` as random variables, and specifies the **prior**. Even if we train the `model` (actually, in this case only the `guide` gets trained, why in *model_4* the `model` gets trained too) to get a good estimate of posterior, the `model` itself is still a `prior` model! (look at the generated data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dist.Normal(2.0, 1.0).sample((100, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    cond_model = pyro.condition(model, data={\"x\": data})\n",
    "    guide = AutoDelta(cond_model)\n",
    "    elbo = Trace_ELBO()\n",
    "    optim = pyro.optim.AdamW({\"lr\": 0.02})\n",
    "    svi = SVI(cond_model, guide, optim, elbo)\n",
    "\n",
    "    # training\n",
    "    losses = []\n",
    "    for i in range(500):\n",
    "        loss = svi.step()\n",
    "        losses.append(loss)\n",
    "    print(\"loss:\")\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "    # show result\n",
    "    rich.print(dict(pyro.get_param_store()))\n",
    "    print(\"model:\")\n",
    "    render_model_plus(cond_model)\n",
    "    print(\"guide:\")\n",
    "    render_model_plus(guide)\n",
    "\n",
    "    print(\"generated data:\")\n",
    "    plt.figure(figsize=(9, 3))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        generated_data = model().detach().numpy()\n",
    "        plt.hist(generated_data, bins=20, density=True)\n",
    "        plt.hist(data.numpy(), bins=20, density=True)\n",
    "        plt.title(f\"mean={round(float(np.mean(generated_data)), 3)}\\n\"  #\n",
    "                  + f\"var={round(float(np.var(generated_data)), 3)}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    For a `model` without random variables (aka. without `pyro.sample`),\n",
    "    and without `pyro.param`:\n",
    "    \n",
    "    - the `model` cannot be trained\n",
    "    - the `guide` contains nothing\n",
    "    - the generated data will always be N(0.0, 1.0)\n",
    "\"\"\"\n",
    "def model_0():\n",
    "    with pyro.plate(\"num\", size=len(data)):\n",
    "        x = pyro.sample(\"x\", dist.Normal(0.0, 1.0))\n",
    "    return x\n",
    "    \n",
    "test(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    For a `model` containing only params\n",
    "    \n",
    "    - the `param` will be trained\n",
    "    - the `guide` is `None`\n",
    "    - the generated data will be deterministic: N(trained_loc, trained_scale)\n",
    "\"\"\"\n",
    "def model_1():\n",
    "    loc = pyro.param(\"loc\", torch.tensor(0.0))\n",
    "    scale = pyro.param(\"scale\", torch.tensor(1.0))\n",
    "    with pyro.plate(\"num\", size=len(data)):\n",
    "        x = pyro.sample(\"x\", dist.Normal(loc, scale))\n",
    "    return x\n",
    "\n",
    "test(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    For a `model` containing random variables\n",
    "    \n",
    "    - the `model` will `NOT` be trained, since no params exist\n",
    "    - the `guide` contains variables (and corresponding params)\n",
    "    - the generated data will be stochastic, although the guide gives a \n",
    "      good estimation of `loc` and `scale`. Note that no params exist in\n",
    "      the model!\n",
    "\n",
    "\"\"\"\n",
    "def model_2():\n",
    "    loc = pyro.sample(\"loc\", dist.Normal(0.0, 1.0))\n",
    "    scale = pyro.sample(\"scale\", dist.LogNormal(0.0, 3.0))\n",
    "    with pyro.plate(\"num\", size=len(data)):\n",
    "        x = pyro.sample(\"x\", dist.Normal(loc, scale))\n",
    "    return x\n",
    "\n",
    "test(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    For a `model` containing both `pyro.sample` and `pyro.param`:\n",
    "    \n",
    "    - the `model` will be trained (but only the `scale` is trained)\n",
    "    - the `guide` contains variables about `loc`, but not about `scale`\n",
    "    - the generated data will be stochastic about `loc` but deterministic\n",
    "      about `scale`. \n",
    "\"\"\"\n",
    "def model_3():\n",
    "    loc = pyro.sample(\"loc\", dist.Normal(0.0, 5.0))\n",
    "    scale = pyro.param(\"scale\", torch.tensor(10.0))\n",
    "    with pyro.plate(\"num\", size=len(data)):\n",
    "        x = pyro.sample(\"x\", dist.Normal(loc, scale))\n",
    "    return x\n",
    "\n",
    "test(model_3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15599cad0da4c5da1b184cfd1e470e7358b141a55537aea656512ec5dac1993"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pyro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
